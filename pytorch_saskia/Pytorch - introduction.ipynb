{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"slide1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"firmen.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"torch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually one uses PyTorch either as:\n",
    "\n",
    "    A replacement for numpy to use the power of GPUs.\n",
    "    a deep learning research platform that provides maximum flexibility and speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a python package that provides two high-level features:\n",
    "\n",
    "- Tensor computation (like numpy) with strong GPU acceleration\n",
    "- Deep Neural Networks built on a tape-based autodiff system\n",
    "\n",
    "You can reuse your favorite python packages such as numpy, scipy and Cython to extend PyTorch when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch, we use a technique called Reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as autograd, autograd, Chainer, etc.\n",
    "\n",
    "<img src=\"graph.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dynamische Aufbau erleichtert Debugging aber ist dafür nicht so perfomant wie statische Varianten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"package.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use numpy, then you have used Tensors (a.k.a ndarray).\n",
    "\n",
    "<img src=\"tensor_illustration.png\">\n",
    "\n",
    "PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerate compute by a huge amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting Regeln siehe: http://pytorch.org/docs/master/notes/broadcasting.html\n",
    "\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensoren\n",
    "\n",
    "PyTorch Tensors verhalten sich sehr ähnlich wie numpy.ndarrays\n",
    "\n",
    "torch.Tensor is an alias for the default tensor type (torch.FloatTensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-37 *\n",
      "       [[ 3.4812,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6274,  0.7488,  0.5962],\n",
      "        [ 0.6923,  0.0584,  0.5398],\n",
      "        [ 0.9051,  0.9494,  0.0254],\n",
      "        [ 0.5413,  0.6173,  0.9948],\n",
      "        [ 0.9108,  0.8124,  0.4479]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0308],\n",
      "        [ 0.3140],\n",
      "        [ 0.8772],\n",
      "        [ 0.0796],\n",
      "        [ 0.4638]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0194,  0.0231,  0.0184],\n",
      "        [ 0.2174,  0.0183,  0.1695],\n",
      "        [ 0.7939,  0.8327,  0.0223],\n",
      "        [ 0.0431,  0.0491,  0.0791],\n",
      "        [ 0.4224,  0.3767,  0.2077]])\n"
     ]
    }
   ],
   "source": [
    "z = y * x \n",
    "print (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6]])\n",
      "tensor([[  1,   4,   9],\n",
      "        [ 16,  25,  36]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = a.pow(2)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tensor.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1],\n",
      "        [ 2,  3]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones((2,), dtype=torch.int8)\n",
    "data = [[0, 1], [2, 3]]\n",
    "print(tensor.new_tensor(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "- Automatische Ableitungen\n",
    "- Keine Session - der Code bestimmt den Graphen (define-by-run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable\n",
    "\n",
    "![](http://pytorch.org/tutorials/_images/Variable.png)\n",
    "\n",
    "- Wrapper für Tensoren\n",
    "- zentrale Schnittstelle zu pyTorch\n",
    "- hält Methoden für die Bearbeitung der Gradienten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.,  1.]]])\n"
     ]
    }
   ],
   "source": [
    "#x = torch.autograd.Variable(torch.ones(2, 2), requires_grad=True)   Depricated\n",
    "x = torch.ones((2, 3, 4), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.,  3.,  3.,  3.],\n",
      "         [ 3.,  3.,  3.,  3.],\n",
      "         [ 3.,  3.,  3.,  3.]],\n",
      "\n",
      "        [[ 3.,  3.,  3.,  3.],\n",
      "         [ 3.,  3.,  3.,  3.],\n",
      "         [ 3.,  3.,  3.,  3.]]])\n"
     ]
    }
   ],
   "source": [
    "f = x + 2\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 5)  # requires_grad=False by default\n",
    "y = torch.randn(5, 5)  # requires_grad=False by default\n",
    "z = torch.randn((5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "print(a.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "b = a + z\n",
    "print(b.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netzwerke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [```torch.nn```](http://pytorch.org/docs/master/nn.html)\n",
    "- ```nn.Module```: \n",
    "    - Basisklasse aller Neuronalen Netzwerke\n",
    "- ```nn.Parameter```: \n",
    "    - Ähnlich wie ```autograd.Variable```, registriert sich automatisch innerhalb einer von ```nn.Module``` erbenden Klasse\n",
    "    - Alle definierten Parameter können über das Attribut ```net.parameters``` ausgegeben werden\n",
    "- ```autograd.Function```: \n",
    "    - Implementierung der Vorwärts-Pfade im NN. \n",
    "    - Über die Registrierung von ```nn.Variable```  in F representiert diese mindestens 1 Knoten im Graphen des NN\n",
    "    - Hier wird der Pfad und die Werte gespeichert um Backpropagation durchführen zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "    Define the neural network that has some learnable parameters (or weights)\n",
    "    Iterate over a dataset of inputs\n",
    "    Process input through the network\n",
    "    Compute the loss \n",
    "    Propagate gradients back into the network’s parameters\n",
    "    Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n",
    "    \n",
    "    \n",
    " Can calculate: \n",
    "     - loss\n",
    "     - backpropagation\n",
    "     - update weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0743,  0.1011,  0.0975,  0.0236,  0.0713,  0.0882, -0.0971,\n",
      "         -0.0216,  0.0827,  0.0792]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
